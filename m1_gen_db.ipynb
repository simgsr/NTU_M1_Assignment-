{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88e05d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV...\n",
      "Parsing categories...\n",
      "  Processed 500,000 rows...\n",
      "  Processed 1,000,000 rows...\n",
      "Creating categories lookup table...\n",
      "Creating job-categories junction table...\n",
      "\n",
      "=== DATA LOADED ===\n",
      "Jobs DataFrame: Shape = (1048585, 21)\n",
      "Categories DataFrame: Shape = (43, 2)\n",
      "JobCategories DataFrame: Shape = (1767829, 2)\n",
      "Jobs columns: ['employmentTypes', 'metadata_expiryDate', 'metadata_isPostedOnBehalf', 'metadata_jobPostId', 'metadata_newPostingDate', 'metadata_originalPostingDate', 'metadata_repostCount', 'metadata_totalNumberJobApplication', 'metadata_totalNumberOfView', 'minimumYearsExperience']...\n",
      "\n",
      "Jobs unique value counts per column:\n",
      "  employmentTypes: 8\n",
      "  metadata_expiryDate: 453\n",
      "  metadata_isPostedOnBehalf: 2\n",
      "  metadata_jobPostId: 1044597\n",
      "  metadata_newPostingDate: 431\n",
      "  metadata_originalPostingDate: 603\n",
      "  metadata_repostCount: 3\n",
      "  metadata_totalNumberJobApplication: 376\n",
      "  metadata_totalNumberOfView: 1552\n",
      "  minimumYearsExperience: 50\n",
      "  numberOfVacancies: 108\n",
      "  occupationId: 0\n",
      "  positionLevels: 9\n",
      "  postedCompany_name: 53151\n",
      "  salary_maximum: 2621\n",
      "  salary_minimum: 2138\n",
      "  salary_type: 1\n",
      "  status_id: 1\n",
      "  status_jobStatus: 3\n",
      "  title: 377084\n",
      "  average_salary: 4277\n",
      "\n",
      "Categories unique value counts per column:\n",
      "  category_id: 43\n",
      "  category_name: 43\n",
      "\n",
      "JobCategories unique value counts per column:\n",
      "  job_id: 1044597\n",
      "  category_id: 43\n",
      "\n",
      "Saving to DuckDB...\n",
      "Saving to Pickle...\n",
      "\n",
      "ETL pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Job Postings ETL Pipeline\n",
    "\n",
    "This script processes a large CSV file with nested JSON fields containing category objects,\n",
    "normalizes the categories column, and persists the data to DuckDB and Pickle formats.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# 1. User Input Handling\n",
    "# ============================================================================\n",
    "\n",
    "def get_file_path():\n",
    "    \"\"\"Prompt user for CSV file path.\"\"\"\n",
    "    file_path = input(\"Enter the path to the CSV file: \").strip()\n",
    "    file_path = file_path.strip('\"').strip(\"'\")\n",
    "    return file_path\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Data Loading Function\n",
    "# ============================================================================\n",
    "\n",
    "def load_csv_efficiently(file_path):\n",
    "    \"\"\"Load CSV with optimized dtypes for memory efficiency.\"\"\"\n",
    "    print(\"Loading CSV...\")\n",
    "    \n",
    "    # First pass: read dtypes from sample\n",
    "    sample = pd.read_csv(file_path, nrows=1000)\n",
    "    \n",
    "    dtype_dict = {}\n",
    "    for col in sample.columns:\n",
    "        if col == 'categories':\n",
    "            dtype_dict[col] = 'object'\n",
    "        elif sample[col].dtype == 'object':\n",
    "            if sample[col].nunique() / len(sample) < 0.5:\n",
    "                dtype_dict[col] = 'category'\n",
    "            else:\n",
    "                dtype_dict[col] = 'object'\n",
    "        elif sample[col].dtype == 'int64':\n",
    "            dtype_dict[col] = 'int32'\n",
    "        elif sample[col].dtype == 'float64':\n",
    "            dtype_dict[col] = 'float32'\n",
    "    \n",
    "    # Read full CSV with optimized dtypes\n",
    "    df = pd.read_csv(file_path, dtype=dtype_dict)\n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Category Normalization Function\n",
    "# ============================================================================\n",
    "\n",
    "def parse_categories(json_str):\n",
    "    \"\"\"\n",
    "    Parse JSON string to extract category id and name.\n",
    "    Expected format: [{\"id\": 13, \"category\": \"Environment / Health\"}, ...]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if pd.isna(json_str):\n",
    "            return []\n",
    "        \n",
    "        data = json.loads(json_str)\n",
    "        \n",
    "        if isinstance(data, list):\n",
    "            categories = []\n",
    "            for item in data:\n",
    "                if isinstance(item, dict):\n",
    "                    # Extract id and category fields\n",
    "                    cat_id = item.get('id')\n",
    "                    cat_name = item.get('category')\n",
    "                    \n",
    "                    if cat_id is not None and cat_name is not None:\n",
    "                        categories.append({\n",
    "                            'category_id': int(cat_id),  # Keep as int for efficiency\n",
    "                            'category_name': str(cat_name).strip()\n",
    "                        })\n",
    "            return categories\n",
    "        return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def normalize_categories(df, job_id_col='metadata_jobPostId'):\n",
    "    \"\"\"Normalize the categories column into lookup and junction tables.\"\"\"\n",
    "    \n",
    "    # Create jobs table (drop categories column)\n",
    "    jobs_df = df.drop(columns=['categories'])\n",
    "    \n",
    "    # Parse all categories\n",
    "    print(\"Parsing categories...\")\n",
    "    all_categories = []\n",
    "    job_category_links = []\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    for idx, row in df.iterrows():\n",
    "        job_id = row[job_id_col]\n",
    "        categories = parse_categories(row['categories'])\n",
    "        \n",
    "        for cat in categories:\n",
    "            # Store for categories lookup\n",
    "            all_categories.append((cat['category_id'], cat['category_name']))\n",
    "            # Store for junction table\n",
    "            job_category_links.append((job_id, cat['category_id']))\n",
    "        \n",
    "        # Progress indicator (every 500k rows)\n",
    "        if idx > 0 and idx % 500000 == 0:\n",
    "            print(f\"  Processed {idx:,} rows...\")\n",
    "    \n",
    "    # Create categories lookup table\n",
    "    print(\"Creating categories lookup table...\")\n",
    "    unique_categories = list(set(all_categories))  # Remove duplicates\n",
    "    categories_df = pd.DataFrame(unique_categories, columns=['category_id', 'category_name'])\n",
    "    categories_df = categories_df.sort_values('category_id').reset_index(drop=True)\n",
    "    \n",
    "    # Create job_categories junction table\n",
    "    print(\"Creating job-categories junction table...\")\n",
    "    job_categories_df = pd.DataFrame(job_category_links, columns=['job_id', 'category_id'])\n",
    "    job_categories_df = job_categories_df.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # Optimize dtypes\n",
    "    categories_df['category_id'] = categories_df['category_id'].astype('int32')\n",
    "    categories_df['category_name'] = categories_df['category_name'].astype('category')\n",
    "    \n",
    "    job_categories_df['job_id'] = job_categories_df['job_id'].astype('category')\n",
    "    job_categories_df['category_id'] = job_categories_df['category_id'].astype('int32')\n",
    "    \n",
    "    return jobs_df, categories_df, job_categories_df\n",
    "\n",
    "def load_and_normalize():\n",
    "    \"\"\"Main function to load and normalize data.\"\"\"\n",
    "    file_path = get_file_path()\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    df = load_csv_efficiently(file_path)\n",
    "    \n",
    "    if 'categories' not in df.columns:\n",
    "        print(\"Error: 'categories' column not found in CSV\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    jobs_df, categories_df, job_categories_df = normalize_categories(df)\n",
    "    \n",
    "    output_dir = os.path.dirname(file_path)\n",
    "    return jobs_df, categories_df, job_categories_df, output_dir\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Output Persistence Functions\n",
    "# ============================================================================\n",
    "\n",
    "def save_to_duckdb(jobs_df, categories_df, job_categories_df, output_dir):\n",
    "    \"\"\"Save DataFrames to DuckDB database.\"\"\"\n",
    "    db_path = os.path.join(output_dir, 'job_postings.db')\n",
    "    conn = duckdb.connect(db_path)\n",
    "    \n",
    "    # Create tables\n",
    "    conn.execute(\"CREATE OR REPLACE TABLE jobs AS SELECT * FROM jobs_df\")\n",
    "    conn.execute(\"CREATE OR REPLACE TABLE categories AS SELECT * FROM categories_df\")\n",
    "    conn.execute(\"CREATE OR REPLACE TABLE job_categories AS SELECT * FROM job_categories_df\")\n",
    "    \n",
    "    # Create indexes\n",
    "    if len(jobs_df) > 0:\n",
    "        conn.execute(\"CREATE INDEX idx_job_id ON jobs(metadata_jobPostId)\")\n",
    "    if len(job_categories_df) > 0:\n",
    "        conn.execute(\"CREATE INDEX idx_jc_job_id ON job_categories(job_id)\")\n",
    "        conn.execute(\"CREATE INDEX idx_jc_category_id ON job_categories(category_id)\")\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "def save_to_pickle(jobs_df, categories_df, job_categories_df, output_dir):\n",
    "    \"\"\"Save DataFrames to Pickle files.\"\"\"\n",
    "    base_path = os.path.join(output_dir, 'job_postings')\n",
    "    \n",
    "    jobs_df.to_pickle(f'{base_path}_jobs.pkl', compression=None)\n",
    "    categories_df.to_pickle(f'{base_path}_categories.pkl', compression=None)\n",
    "    job_categories_df.to_pickle(f'{base_path}_job_categories.pkl', compression=None)\n",
    "\n",
    "# ============================================================================\n",
    "# 5. Data Validation Display Function\n",
    "# ============================================================================\n",
    "\n",
    "def display_validation_info(jobs_df, categories_df, job_categories_df):\n",
    "    \"\"\"Display only the required validation information.\"\"\"\n",
    "    \n",
    "    print(\"\\n=== DATA LOADED ===\")\n",
    "    \n",
    "    # Display shapes\n",
    "    print(f\"Jobs DataFrame: Shape = {jobs_df.shape}\")\n",
    "    print(f\"Categories DataFrame: Shape = {categories_df.shape}\")\n",
    "    print(f\"JobCategories DataFrame: Shape = {job_categories_df.shape}\")\n",
    "    \n",
    "    # Display Jobs columns (first 10 only)\n",
    "    cols_list = jobs_df.columns.tolist()\n",
    "    cols_str = str(cols_list[:10]) + ('...' if len(cols_list) > 10 else '')\n",
    "    print(f\"Jobs columns: {cols_str}\")\n",
    "    print()\n",
    "    \n",
    "    # Display unique counts for Jobs table\n",
    "    print(\"Jobs unique value counts per column:\")\n",
    "    for col in jobs_df.columns:\n",
    "        try:\n",
    "            unique_count = jobs_df[col].nunique()\n",
    "            print(f\"  {col}: {unique_count}\")\n",
    "        except:\n",
    "            print(f\"  {col}: Error\")\n",
    "    print()\n",
    "    \n",
    "    # Display unique counts for Categories table\n",
    "    if len(categories_df) > 0:\n",
    "        print(\"Categories unique value counts per column:\")\n",
    "        for col in categories_df.columns:\n",
    "            try:\n",
    "                unique_count = categories_df[col].nunique()\n",
    "                print(f\"  {col}: {unique_count}\")\n",
    "            except:\n",
    "                print(f\"  {col}: Error\")\n",
    "    else:\n",
    "        print(\"Categories table is empty\")\n",
    "    print()\n",
    "    \n",
    "    # Display unique counts for JobCategories table\n",
    "    if len(job_categories_df) > 0:\n",
    "        print(\"JobCategories unique value counts per column:\")\n",
    "        for col in job_categories_df.columns:\n",
    "            try:\n",
    "                unique_count = job_categories_df[col].nunique()\n",
    "                print(f\"  {col}: {unique_count}\")\n",
    "            except:\n",
    "                print(f\"  {col}: Error\")\n",
    "    else:\n",
    "        print(\"JobCategories table is empty\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. Main Execution\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main ETL pipeline execution.\"\"\"\n",
    "    \n",
    "    jobs_df, categories_df, job_categories_df, output_dir = load_and_normalize()\n",
    "    \n",
    "    if jobs_df is None:\n",
    "        print(\"Failed to load data. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    display_validation_info(jobs_df, categories_df, job_categories_df)\n",
    "    \n",
    "    print(\"\\nSaving to DuckDB...\")\n",
    "    save_to_duckdb(jobs_df, categories_df, job_categories_df, output_dir)\n",
    "    \n",
    "    print(\"Saving to Pickle...\")\n",
    "    save_to_pickle(jobs_df, categories_df, job_categories_df, output_dir)\n",
    "    \n",
    "    print(\"\\nETL pipeline completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
